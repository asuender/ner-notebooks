{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE to NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Andreas Sünder & Benjamin Kissinger\n",
    "\n",
    "Das Ziel dieses Notebooks ist ein normales Dataset zu tokenizen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes importieren wir alle notwendigen Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben10/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True) # Spezieller Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anschließend speichern wir das Dataset in ein Python Objekt. In diesem Fall importieren wir es von HuggingFace, jedoch kann man es auch lokal laden. Zusätzlich definieren wir gleich, welchen NER-Tag wir welcher Zahl zuweisen wollen. In diesem Fall wollen wir Autoren und Daten erkennen. Daher definieren wir: O==>0; Author==>1; Date==>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.41M/1.41M [00:00<00:00, 1.88MB/s]\n",
      "Downloading data: 100%|██████████| 353k/353k [00:00<00:00, 878kB/s]s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.71it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 713.68it/s]\n",
      "Generating train split: 6361 examples [00:00, 202228.21 examples/s]\n",
      "Generating validation split: 1591 examples [00:00, 195366.62 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"textminr/ner_extended\")\n",
    "ner_tag_to_id = {\"O\": 0, \"AUTHOR\": 1, \"DATE\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die anschließende Funktion erledigt prinzipell die ganze Arbeit. Im Anschluss wird diese genauer erklärt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6361/6361 [00:03<00:00, 1732.02 examples/s]\n",
      "Map: 100%|██████████| 1591/1591 [00:01<00:00, 1418.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_row(row):\n",
    "    prompt = row[\"prompt\"]\n",
    "    response = json.loads(row[\"response\"])\n",
    "    author = word_tokenize(response[\"author\"])\n",
    "    date = response[\"date\"]\n",
    "\n",
    "    tokens = word_tokenize(prompt)\n",
    "    ner_ids = []\n",
    "    ner_tags = []\n",
    "\n",
    "    counter_author = 0\n",
    "    counter_date = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in author and counter_author < len(author):\n",
    "            ner_ids.append(ner_tag_to_id[\"AUTHOR\"])\n",
    "            ner_tags.append(\"AUTHOR\")\n",
    "            counter_author += 1\n",
    "        elif token in date and counter_date < 1:\n",
    "            ner_ids.append(ner_tag_to_id[\"DATE\"])\n",
    "            ner_tags.append(\"DATE\")\n",
    "            counter_date += 1\n",
    "        else:\n",
    "            ner_ids.append(ner_tag_to_id[\"O\"])\n",
    "            ner_tags.append(\"O\")\n",
    "\n",
    "    row[\"tokens\"] = tokens\n",
    "    row[\"ner_ids\"] = ner_ids\n",
    "    row[\"ner_tags\"] = ner_tags\n",
    "\n",
    "    return row\n",
    "\n",
    "dataset_tokenized = dataset.map(convert_row).remove_columns([\"prompt\", \"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Entry aus dem Ursprungsdatensatz ist wie folgt aufgebaut:\n",
    "\n",
    "*{\"prompt\": \"This text was written by Robert Smith on the 4th of July, 2020.\", \"response\": \"{\\\"author\\\": \\\"Robert Smith\\\", \\\"date\\\": \\\"2020\\\"}\"}*\n",
    "\n",
    "Im Prinzip müssen wir jetzt 2 Listen erstellen, welche jeweils entweder die NER_IDS oder die NER_TAGS speichern.\n",
    "\n",
    "Wir speichern nun alle Elemente dieses Datasets in einer Variable und zusätzlich tokenizen wir den Autor und den Prompt. Das Datum müssen wir nicht tokenizen, da dieses immer nur aus einer Jahreszahl besteht und ergo ein Element ist. \n",
    "\n",
    "Nun gehen wir die ganzen Tokens des Prompts durch und setzen jeden Token auf 1, wenn dieser im Autor vorkommt, jeden Token auf 2, wenn dieser im Date vorkommt und alle anderen werden auf 0 gesetzt, da diese für dieses Modell irrelevant sind (weil wir eben nur Autoren und Daten klassifizieren wollen). Die erstellten Liste fügen wir jetzt als neue Spalten pro Zeile ein. \n",
    "\n",
    "Den Rest erledigt die **map**-Methode für uns. Diese wendet die mitgegebene Methode für alle Reihen eines datasets an. Also wird der oben beschriebene Prozess an allen Zeilen angewandt. Zusätzlich werden die Spalten \"prompt\" und \"response\" gelöscht, da wir diese nicht mehr für den NER-Classifier benötigen. \n",
    "\n",
    "Anschließend pushen wir den neuen Datensatz noch auf HuggingFace (optional, nicht vergessen den Token zu setzen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
