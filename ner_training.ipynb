{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning von BERT f√ºr Named Entity Recognition (NER)\n",
    "\n",
    "von Andreas S√ºnder & Benjamin Kissinger (2024)\n",
    "\n",
    "Ziel dieses Tutorials ist es, ein BERT-Modell f√ºr Named Entity Recognition (NER) zu trainieren. Das fertige Modell soll f√ºr das Extrahieren von Metadaten aus B√ºchern, Literatur etc. verwendet werden k√∂nnen. Am Ende ist es in der Lage, aus einem Ausgangstext den Autor sowie das Ver√∂ffentlichungsjahr zu extrahieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Zun√§chst sind die notwendigen Packages zu installieren:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden des Datasets\n",
    "\n",
    "F√ºr dieses Tutorial nutzen wir diverse Libraries, die von *ü§ó HuggingFace* via Python bereitgestellt werden, darunter finden sich *ü§ó Datasets* und *ü§ó Transformers*. Ersteres ist zum Interagieren mit Datens√§tzen, die auf der Platform *HuggingFace Hub* (https://huggingface.co/) frei zur Verf√ºgung gestellt werden. Die Transformers-Library kann zum Interagieren (sprich Trainieren etc.) mit diversen Modellen verwendet werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zun√§chst laden wir den passenden Datensatz, der unter https://huggingface.co/datasets/textminr/ner_tokenized verf√ºgbar ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('textminr/ner_tokenized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorbereitung des Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danach k√∂nnen wir den *Tokenizer* laden. Zu jedem Sprachmodell gibt es einen passenden Tokenizer, der die Eingabe in die passenden Tokens (das sind kleinere Einheiten bzw. Subw√∂rter) aufsplittet. Hier m√ºssen wir auch die Modellbezeichnung angeben, die wir verwenden wollen. In unserem Fall ist das `bert-base-multilingual-cased`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'bert-base-multilingual-cased', # ID des Modells, um den richtigen Tokenizer zu laden\n",
    "  add_prefix_space=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun m√ºssen wir unsere Trainingsdaten aus dem Dataset *tokenizen*, sprich jeden Satz in kleinere Einheiten aufteilen. Daf√ºr verwenden wir eine eigene Funktion `tokenize_and_align_labels`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(row):\n",
    "  tokenized_inputs = tokenizer(row['tokens'], truncation=True, is_split_into_words=True)\n",
    "\n",
    "  labels = []\n",
    "  for i, label in enumerate(row[f'ner_ids']):\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "      if word_idx is None:\n",
    "        label_ids.append(-100)\n",
    "      elif word_idx != previous_word_idx:\n",
    "        label_ids.append(label[word_idx])\n",
    "      else:\n",
    "        label_ids.append(-100)\n",
    "      previous_word_idx = word_idx\n",
    "    labels.append(label_ids)\n",
    "\n",
    "  tokenized_inputs['labels'] = labels\n",
    "  return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beim Trainieren von Sprachmodellen ist es immer wichtig, dass **alle** S√§tze die gleiche L√§nge haben. Daf√ºr m√ºssen wir die S√§tze *padden*, sprich die k√ºrzeren S√§tze mit einem speziellen Token (am Anfang oder Ende) auff√ºllen. Dazu verwenden wir einen *DataCollator*, welchen wir sp√§ter beim Trainieren mitgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um bewerten zu k√∂nnen, ob unser Modell \"gut\" ist, k√∂nnen wir w√§hrend des Trainings die *Accuracy* berechnen. Nachdem in den Trainingsdaten die Labels f√ºr jedes Token enthalten sind, k√∂nnen wir die Accuracy berechnen, indem wir die Anzahl der richtigen Vorhersagen mit den wahren Labels vergleichen. Daf√ºr verwenden wir eine eigene Funktion `compute_metrics`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load('seqeval')\n",
    "import numpy as np\n",
    "\n",
    "# Labels, die wir extrahieren wollen\n",
    "label_list = ['O', 'AUTHOR', 'DATE']\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "  true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "  ]\n",
    "  true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "  ]\n",
    "\n",
    "  results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "  return {\n",
    "    'precision': results['overall_precision'],\n",
    "    'recall': results['overall_recall'],\n",
    "    'f1': results['overall_f1'],\n",
    "    'accuracy': results['overall_accuracy'],\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir alle Vorkehrungen getroffen, um das Modell zu trainieren! Zu Beginn m√ºssen wir das Modell aber herunterladen und in den Speicher (RAM oder VRAM, wenn eine GPU verf√ºgbar ist) laden. Wir m√ºssen hier auch die Labels der Klassen angeben, die wir extrahieren wollen (wobei jedes Label auch eine Art \"ID\", sprich eine Zahl, hat):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "id2tag= {\n",
    "  0: 'O',\n",
    "  1: 'AUTHOR',\n",
    "  2: 'DATE',\n",
    "}\n",
    "tag2id = {v: k for k, v in id2tag.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "  'bert-base-multilingual-cased', # wieder die ID des Modells\n",
    "  num_labels=len(label_list), # Anzahl der Labels\n",
    "  id2label=id2tag, # Zuordnung der IDs zu den Labels\n",
    "  label2id=tag2id, # Zuordnung der Labels zu den IDs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace Transformers bietet eine einfache M√∂glichkeit an, um unser Modell zu trainieren. Wir definieren hierf√ºr ein Objekt der Klasse `TrainingArguments` und √ºbergeben unsere gew√ºnschten Trainingseinstellungen (auch genannt *Hyperparameter*). Unsere `traininga-args` √ºbergeben wir dann einem `Trainer` mitsamt den Datens√§tzen, der Evaluierungs-Funktion und dem Data Collator von vorhin (und noch ein paar andere Sachen). Das Training starten wir mit `trainer.train()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  output_dir='./output',\n",
    "  per_device_eval_batch_size=4,\n",
    "  per_device_train_batch_size=4,\n",
    "  learning_rate=2e-5,\n",
    "  num_train_epochs=1,\n",
    "  logging_strategy='steps',\n",
    "  logging_steps=25,\n",
    "  evaluation_strategy='epoch',\n",
    "  eval_steps=1,\n",
    "  save_strategy='epoch',\n",
    "  save_steps=1,\n",
    ")\n",
    "  \n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  tokenizer=tokenizer,\n",
    "  train_dataset=tokenized_datasets['train'],\n",
    "  eval_dataset=tokenized_datasets['validation'],\n",
    "  data_collator=data_collator,\n",
    "  compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haben wir unser Modell fertig trainiert, k√∂nnen wir es auch gleich testen. Vorher hatten wir das `output-dir` auf `./output` gesetzt, weshalb wir das Modell nun auch von dort laden (hier muss nur noch der Checkpoint-Ordner angepasst werden):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "  'token-classification',\n",
    "  model='./output/checkpoint-<XXXX>', # Speicherort des Modells\n",
    "  aggregation_strategy='simple'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Alan Walker's vibrant self-portraits, painted in 1940, express her tumultuous inner world.\"\n",
    "classifier(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
